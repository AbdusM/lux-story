# Lux Story Pilot - Success Metrics Framework
**Urban Chamber Pilot - January 2025**

---

## Overview

This document defines how we'll measure success for the Lux Story pilot with Anthony's 16 Birmingham graduates. These metrics balance **quantitative data** (what we can measure) with **qualitative insights** (what participants experienced).

**Critical principle:** Success isn't "did everyone finish?" but "did this reveal valuable patterns and spark career exploration?"

---

## Tier 1: MUST-HAVE Metrics (Pilot Viability)

These determine if the pilot achieved its core purpose.

### 1.1 Engagement Rate
**What:** % of participants who engage meaningfully with the experience

**Measurement:**
- **Minimum engagement:** Completed at least 2 character arcs (Samuel + 1 other)
- **Target:** 70%+ of participants reach minimum engagement
- **Success threshold:** ≥ 60% (acceptable), ≥ 75% (strong), < 50% (red flag)

**Why this matters:** If less than half engage, the format doesn't resonate with Birmingham youth.

**Data source:** Admin dashboard - completion metrics

---

### 1.2 Pattern Discovery
**What:** Did participants discover behavioral patterns?

**Measurement:**
- **Pattern diversity:** Each participant earns orbs in at least 3 of 5 patterns
- **Target:** 80%+ participants have 3+ active patterns
- **Success threshold:** ≥ 70% (acceptable), ≥ 85% (strong), < 60% (red flag)

**Why this matters:** If everyone has identical patterns or only 1-2, the system isn't revealing useful insights.

**Data source:** Admin dashboard - pattern distribution per user

---

### 1.3 Career Insight Generation
**What:** Did participants discover NEW career interests?

**Measurement:**
- Post-pilot survey question: "Did you discover a career interest you hadn't considered before?"
- **Target:** 50%+ say "Yes"
- **Success threshold:** ≥ 40% (acceptable), ≥ 60% (strong), < 30% (red flag)

**Why this matters:** If it's not expanding career horizons, it's just entertainment (which is fine, but not the goal).

**Data source:** Post-pilot survey (Google Form)

---

## Tier 2: NICE-TO-HAVE Metrics (Experience Quality)

These show if the experience was valuable, even if engagement was lower than hoped.

### 2.1 Completion Rate (Full Experience)
**What:** % of participants who complete 4+ character arcs

**Measurement:**
- Track how many finish Samuel + 3 other characters
- **Target:** 40%+ complete 4+ arcs
- **Success threshold:** ≥ 30% (acceptable), ≥ 50% (strong), < 20% (needs work)

**Why this matters:** Shows depth of engagement, but NOT a make-or-break metric (some might get value from 2 arcs).

**Data source:** Admin dashboard

---

### 2.2 Time Spent
**What:** Average time spent in-game

**Measurement:**
- Track total playtime per participant
- **Target:** 1-2 hours average
- **Success threshold:** 45-90 min (acceptable), 90-120 min (strong), < 30 min (red flag)

**Why this matters:** Too little time = didn't engage. Too much time = friction in UX or participants trying to "complete" everything.

**Data source:** Admin dashboard - session duration analytics

---

### 2.3 Mobile Usability
**What:** Did mobile-first design work?

**Measurement:**
- Post-pilot survey: "Did you play primarily on mobile, desktop, or both?"
- **Target:** 60%+ played on mobile
- Follow-up: "How was the mobile experience?" (1-5 scale)
- **Target:** 4.0+ average rating

**Why this matters:** Validates mobile-first design assumption.

**Data source:** Post-pilot survey

---

### 2.4 Self-Recognition Moments
**What:** Did participants have "aha" moments about themselves?

**Measurement:**
- Post-pilot survey: "Was there a moment where you thought 'that's so me'?" (Yes/No)
- **Target:** 60%+ say "Yes"
- Follow-up (open text): "Tell us about that moment."

**Why this matters:** Self-recognition = the pattern system is working.

**Data source:** Post-pilot survey

---

## Tier 3: EXPLORATORY Metrics (Future Development)

These inform future iterations but aren't success/fail criteria for this pilot.

### 3.1 Pattern Distribution (Cohort-Wide)
**What:** What patterns emerge across the Birmingham cohort?

**Measurement:**
- Aggregate pattern scores across all 16 participants
- Look for:
  - Dominant cohort patterns (e.g., "High Helping + Building")
  - Underrepresented patterns (e.g., "Low Analytical")
  - Correlation between patterns and career interests

**Why this matters:** Shows Birmingham-specific behavioral trends. Could inform program design.

**Data source:** Admin dashboard - cohort analytics

---

### 3.2 Character Preferences
**What:** Which characters resonated most?

**Measurement:**
- Track completion rates by character
- Survey question: "Which character's story resonated most with you?"
- **Insight:** Do Birmingham youth prefer certain characters/stories?

**Why this matters:** Guides future content development. If certain characters aren't engaging, revise them.

**Data source:** Admin dashboard + post-pilot survey

---

### 3.3 Drop-Off Points
**What:** Where do participants stop playing?

**Measurement:**
- Identify common exit nodes
- Look for patterns:
  - Do people stop after Samuel intro? (Friction in first experience)
  - Do they stop mid-character arc? (Length issue)
  - Do they stop after 1-2 characters? (Natural stopping point)

**Why this matters:** Identifies UX friction points.

**Data source:** Admin dashboard - navigation flow analytics

---

### 3.4 Debrief Engagement
**What:** Do participants actively discuss patterns in group setting?

**Measurement:**
- Facilitator observation during debrief
- Questions to assess:
  - Did participants share specific examples from their gameplay?
  - Did they connect patterns to career examples unprompted?
  - Did they ask follow-up questions about careers?

**Why this matters:** Group debrief is where individual insights become collective learning.

**Data source:** Facilitator notes

---

## Success Scorecard (Quick Reference)

| Metric | Minimum | Target | Strong | Status |
|--------|---------|--------|--------|--------|
| **Engagement Rate** (2+ arcs) | 60% | 70% | 75%+ | TBD |
| **Pattern Discovery** (3+ patterns) | 70% | 80% | 85%+ | TBD |
| **Career Insight** (new interest) | 40% | 50% | 60%+ | TBD |
| **Completion Rate** (4+ arcs) | 30% | 40% | 50%+ | TBD |
| **Time Spent** (average) | 45 min | 60-90 min | 90-120 min | TBD |
| **Mobile Rating** (1-5) | 3.5 | 4.0 | 4.5+ | TBD |
| **Self-Recognition** (had "aha" moment) | 50% | 60% | 70%+ | TBD |

**Overall Success Criteria:**
- **PASS:** Hit targets on at least 2 of 3 Tier 1 metrics + 3 of 4 Tier 2 metrics
- **STRONG PASS:** Hit targets on all Tier 1 metrics + 3 of 4 Tier 2 metrics
- **NEEDS WORK:** Miss targets on 2+ Tier 1 metrics

---

## Data Collection Methods

### Automated (from game):
- Completion rates per character
- Time spent per session
- Pattern distribution per user
- Navigation paths (where they went, where they stopped)
- Unlock triggers (when abilities activated)

### Manual (post-pilot):
- **Survey** (Google Form sent after 2 weeks):
  - Mobile vs. desktop usage
  - Self-recognition moments
  - Career insights discovered
  - Overall experience rating
  - Open feedback
- **Debrief session** (facilitator notes):
  - Engagement level
  - Quality of discussion
  - Career connections made
  - Questions asked

---

## Red Flags (What to Watch For)

### Engagement Red Flags:
- **< 50% engagement rate** → Format doesn't resonate with Birmingham youth
- **Very high drop-off after Samuel** → First impression fails
- **< 30 min average playtime** → Not enough to discover patterns

### Pattern Red Flags:
- **90%+ participants have identical pattern mix** → System isn't differentiating
- **< 60% achieve 3+ patterns** → Choices aren't diverse enough
- **One pattern dominates across everyone** → Unbalanced choice design

### Career Insight Red Flags:
- **< 30% discover new career interest** → Not achieving exploration goal
- **Participants can't articulate patterns** → System is invisible/confusing
- **No connection between patterns and careers in debrief** → Missing the link

---

## Reporting Format

### Week 3 (End of Pilot):

**Quantitative Report:**
- Engagement metrics (charts, percentages)
- Pattern distribution (cohort-wide heatmap)
- Completion rates by character
- Time spent analysis
- Mobile vs. desktop breakdown

**Qualitative Report:**
- Survey highlights (common themes, standout quotes)
- Debrief observations
- Self-recognition examples
- Career interests discovered

### Debrief Call (Week 4):

**Agenda:**
1. Review quantitative data (15 min)
2. Discuss qualitative insights (20 min)
3. Anthony's observations (15 min)
4. Recommendations for future cohorts (10 min)

---

## Using Metrics to Decide Next Steps

### If Pilot is STRONG PASS:
- **Recommendation:** Scale to more Urban Chamber cohorts
- **Next steps:** Refine based on feedback, create facilitator training, price for B2B
- **Timeline:** Ready for next cohort in 2-3 months

### If Pilot is PASS:
- **Recommendation:** Iterate based on specific feedback, run another pilot
- **Next steps:** Address specific pain points (e.g., mobile UX, character engagement)
- **Timeline:** Iterate for 1-2 months, pilot again

### If Pilot NEEDS WORK:
- **Recommendation:** Major revision or pivot
- **Next steps:** Analyze what failed (format? content? positioning?), redesign
- **Timeline:** 3-6 month revision before next pilot

---

## Questions to Answer Post-Pilot

1. **Did the pattern system work?** (Can participants articulate their patterns?)
2. **Did it spark career exploration?** (Are they curious about new paths?)
3. **Is mobile-first the right format?** (Did Birmingham youth actually play on phones?)
4. **What's the optimal length?** (Is 1-2 hours the right scope, or should it be shorter/longer?)
5. **Which characters matter?** (Can we cut characters that don't resonate?)
6. **Is this scalable?** (Could Anthony run this with future cohorts without you?)
7. **What's the revenue model?** (Is $5-10K per cohort sustainable? What's included?)

---

**These metrics aren't about proving Lux Story is "done"—they're about learning what works for Birmingham youth and how to make it better.**
